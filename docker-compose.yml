# ============================================================
# LLM Enterprise Platform - Docker Compose
# ============================================================
# Stack base del curso "LLMs Locales - Ollama, vLLM y sus Alternativas"
#
# Uso:
#   docker compose up -d                     # Stack base (con vLLM)
#   docker compose --profile ollama up -d    # Stack + Ollama
#   docker compose --profile ollama down     # Parar todo
# ============================================================

services:

  # ===========================================================
  # GATEWAY
  # ===========================================================

  nginx:
    image: nginx:alpine
    container_name: llm-nginx
    restart: unless-stopped
    ports:
      - "80:80"
    volumes:
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      api:
        condition: service_started
      qdrant:
        condition: service_started
    networks:
      - llm-network

  # ===========================================================
  # API BASE
  # ===========================================================

  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: llm-api
    restart: unless-stopped
    expose:
      - "8000"
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    networks:
      - llm-network

  # ===========================================================
  # vLLM - Motor de inferencia principal
  # ===========================================================
  # Requiere NVIDIA GPU con drivers y nvidia-container-toolkit.
  # En maquinas sin GPU, este contenedor fallara al arrancar.
  # Eso es esperado: revisa los logs con `docker logs llm-vllm`.
  # ===========================================================

  vllm:
    image: vllm/vllm-openai:latest
    container_name: llm-vllm
    restart: "no"
    expose:
      - "8000"
    volumes:
      - vllm_cache:/root/.cache/huggingface
    environment:
      - HF_TOKEN=${HF_TOKEN:-}
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
    command: >
      --model ${VLLM_MODEL:-Qwen/Qwen2-1.5B-Instruct-AWQ}
      --max-model-len ${VLLM_MAX_MODEL_LEN:-8192}
      --quantization awq
      --dtype half
      --host 0.0.0.0
      --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - llm-network

  # ===========================================================
  # Ollama - Motor de inferencia alternativo (perfil: ollama)
  # ===========================================================
  # Activar con: docker compose --profile ollama up -d
  # Funciona en cualquier maquina (CPU, Apple Silicon, GPU).
  # ===========================================================

  ollama:
    image: ollama/ollama:latest
    container_name: llm-ollama
    restart: unless-stopped
    profiles:
      - ollama
    expose:
      - "11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./ollama/entrypoint.sh:/entrypoint.sh:ro
    environment:
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen2:1.5b}
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - llm-network

  # ===========================================================
  # QDRANT - Base de datos vectorial
  # ===========================================================

  qdrant:
    image: qdrant/qdrant:latest
    container_name: llm-qdrant
    restart: unless-stopped
    expose:
      - "6333"
      - "6334"
    volumes:
      - qdrant_data:/qdrant/storage
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:6333/healthz"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    networks:
      - llm-network

  # ===========================================================
  # LANGFUSE v3 - Observabilidad LLM
  # ===========================================================

  langfuse-web:
    image: langfuse/langfuse:3
    container_name: llm-langfuse-web
    restart: unless-stopped
    depends_on:
      langfuse-postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      redis:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    ports:
      - "3000:3000"
    expose:
      - "3000"
    environment: &langfuse-env
      DATABASE_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@langfuse-postgres:5432/${POSTGRES_DB:-langfuse}
      NEXTAUTH_URL: ${NEXTAUTH_URL:-http://localhost:3000}
      NEXTAUTH_SECRET: ${NEXTAUTH_SECRET:-mysecret-changeme-in-production}
      SALT: ${SALT:-mysalt-changeme-in-production}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY:-0000000000000000000000000000000000000000000000000000000000000000}
      TELEMETRY_ENABLED: "true"
      LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: "false"
      # ClickHouse
      CLICKHOUSE_MIGRATION_URL: clickhouse://clickhouse:9000
      CLICKHOUSE_URL: http://clickhouse:8123
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-clickhouse}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-clickhouse}
      CLICKHOUSE_CLUSTER_ENABLED: "false"
      # Redis
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_AUTH: ${REDIS_AUTH:-myredissecret}
      REDIS_TLS_ENABLED: "false"
      # MinIO / S3
      LANGFUSE_S3_EVENT_UPLOAD_BUCKET: langfuse
      LANGFUSE_S3_EVENT_UPLOAD_REGION: auto
      LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minio}
      LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-miniosecret}
      LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT: http://minio:9000
      LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE: "true"
      LANGFUSE_S3_EVENT_UPLOAD_PREFIX: "events/"
      LANGFUSE_S3_MEDIA_UPLOAD_BUCKET: langfuse
      LANGFUSE_S3_MEDIA_UPLOAD_REGION: auto
      LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minio}
      LANGFUSE_S3_MEDIA_UPLOAD_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-miniosecret}
      LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT: http://localhost:9090
      LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE: "true"
      LANGFUSE_S3_MEDIA_UPLOAD_PREFIX: "media/"
      LANGFUSE_S3_BATCH_EXPORT_ENABLED: "false"
      LANGFUSE_USE_AZURE_BLOB: "false"
    networks:
      - llm-network

  langfuse-worker:
    image: langfuse/langfuse-worker:3
    container_name: llm-langfuse-worker
    restart: unless-stopped
    depends_on:
      langfuse-postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      redis:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    expose:
      - "3030"
    environment:
      <<: *langfuse-env
    networks:
      - llm-network

  # ===========================================================
  # LANGFUSE - Dependencias de infraestructura
  # ===========================================================

  langfuse-postgres:
    image: postgres:17-alpine
    container_name: llm-langfuse-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-langfuse}
      TZ: UTC
      PGTZ: UTC
    volumes:
      - langfuse_postgres_data:/var/lib/postgresql/data
    expose:
      - "5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 3s
      timeout: 3s
      retries: 10
    networks:
      - llm-network

  clickhouse:
    image: clickhouse/clickhouse-server
    container_name: llm-clickhouse
    restart: unless-stopped
    user: "101:101"
    environment:
      CLICKHOUSE_DB: default
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-clickhouse}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-clickhouse}
    volumes:
      - langfuse_clickhouse_data:/var/lib/clickhouse
      - langfuse_clickhouse_logs:/var/log/clickhouse-server
    expose:
      - "8123"
      - "9000"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8123/ping"]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 1s
    networks:
      - llm-network

  redis:
    image: redis:7
    container_name: llm-redis
    restart: unless-stopped
    command: >
      --requirepass ${REDIS_AUTH:-myredissecret}
      --maxmemory-policy noeviction
    expose:
      - "6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 3s
      timeout: 10s
      retries: 10
    networks:
      - llm-network

  minio:
    image: cgr.dev/chainguard/minio
    container_name: llm-minio
    restart: unless-stopped
    entrypoint: sh
    command: -c 'mkdir -p /data/langfuse && minio server --address ":9000" --console-address ":9001" /data'
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-miniosecret}
    ports:
      - "9090:9000"
    expose:
      - "9000"
      - "9001"
    volumes:
      - langfuse_minio_data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 1s
      timeout: 5s
      retries: 5
      start_period: 1s
    networks:
      - llm-network

# ===========================================================
# VOLUMES
# ===========================================================

volumes:
  vllm_cache:
    driver: local
  ollama_data:
    driver: local
  qdrant_data:
    driver: local
  langfuse_postgres_data:
    driver: local
  langfuse_clickhouse_data:
    driver: local
  langfuse_clickhouse_logs:
    driver: local
  langfuse_minio_data:
    driver: local

# ===========================================================
# NETWORK
# ===========================================================

networks:
  llm-network:
    driver: bridge
